# my global config
global:
  scrape_interval:     15s # By default, scrape targets every 15 seconds.
  evaluation_interval: 15s # By default, scrape targets every 15 seconds.
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  external_labels:
      monitor: 'codelab-monitor'

# Load and evaluate rules in this file every  gc'evaluation_interval' seconds.
rule_files:
  - "rules/alert.rules_nodes"
  - "rules/alert.rules_containers"
  - "rules/alert.rules_container-groups"
  - "rules/alert.rules_sites"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.

#  - job_name: 'node'
#    scrape_interval: 10s
#    scrape_timeout: 5s
#    static_configs:
#      - targets: ['master-nodeexporter:9100']
#        labels: {'host': 'server'}

#  - job_name: 'containers'
#    scrape_interval: 10s
#    scrape_timeout: 5s
#    static_configs:
#      - targets: ['master-cadvisor:8080']
#        labels: {'host': 'server'}

  - job_name: 'pushgateway'
    scrape_interval: 10s
    scrape_timeout: 5s
    static_configs:
      - targets: ['192.168.0.189:9091']
        labels: {'host': 'server'}

  - job_name: 'chirpstack-c1-application'
    scrape_interval: 10s
    scrape_timeout: 5s
    static_configs:
      - targets: ['192.168.0.175:9095']
        labels: {'Cluster': 'fog01'}

  - job_name: 'chirpstack-c1-networkserver'
    scrape_interval: 10s
    scrape_timeout: 5s
    static_configs:
      - targets: ['192.168.0.175:9096']
        labels: {'Cluster': 'fog01'}

  - job_name: 'chirpstack-c1-gatewayserver'
    scrape_interval: 10s
    scrape_timeout: 5s
    static_configs:
      - targets: ['192.168.0.175:9097']
        labels: {'Cluster': 'fog01'}

  - job_name: 'chirpstack-simulator'
    scrape_interval: 10s
    scrape_timeout: 5s
    static_configs:
      - targets: ['192.168.0.175:9099']
        labels: {'Cluster': 'fog01'}


  - job_name: 'prometheus-server'
    scrape_interval: 10s
    scrape_timeout: 5s
    static_configs:
      - targets: ['localhost:9090']
        labels: {'host': 'host1'}

  # see https://github.com/prometheus/blackbox_exporter relabel 
  - job_name: 'service'
    scrape_interval: 60s
    scrape_timeout: 15s
    metrics_path: /probe
    # if your target is https, you either need to install cert in blackbox proble container
    # or add below line to ignore verify
    # tls_config:
    #  insecure_skip_verify: true
    params:
      module: [http_2xx]  # Look for a HTTP 200 response. 
    file_sd_configs:
      - files:
        - /etc/prometheus/service.yml
    relabel_configs:
      - source_labels: [__address__]
        regex: (.*)
        target_label: __param_target
        replacement: ${1}
      - source_labels: [__address__]
        regex: (.*)
        target_label: service_url
        replacement: ${1}
      - source_labels: []
        regex: .*
        target_label: __address__
        replacement: blackboxprober:9115

  # - job_name: 'couchdb'
  #   scrape_interval: 10s
  #   scrape_timeout: 5s
  #   static_configs:
  #     - targets: ['couchdbstats:9984']
  #       labels: {'host': 'host1'}
  # Will/23.08.16: switched of, as I am not sure at this point where the new /_stats endpoint for couchdb 2.0 is. this image is still checking at /_stats and will thus throw non-stop MONITORING SERVICE DOWN alerts. See corresponding entry in monitoring docker-compose.yml.



  # - job_name: 'prometheus'

  #   # Override the global default and scrape targets from this job every 5 seconds.
  #   scrape_interval: 10s
  #   scrape_timeout: 5s

  #   # metrics_path defaults to '/metrics'
  #   # scheme defaults to 'http'.

  #   target_groups:
  #     - targets: ['localhost:9090']
  #       labels: {'host': 'prometheus'}


  - job_name: kubernetes-cadvisor
    scrape_interval: 30s
    scrape_timeout: 10s
    metrics_path: /metrics
    scheme: http
    kubernetes_sd_configs:
    - api_server: http://192.168.0.175:8081
      role: node
      tls_config:
        insecure_skip_verify: true
    tls_config:
      insecure_skip_verify: true
    relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      # Only for Kubernetes ^1.7.3.
      # See: https://github.com/prometheus/prometheus/issues/2916
      - target_label: __address__
        replacement: 192.168.0.175:8081
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
    metric_relabel_configs:
      - action: replace
        source_labels: [id]
        regex: '^/machine\.slice/machine-rkt\\x2d([^\\]+)\\.+/([^/]+)\.service$'
        target_label: rkt_container_name
        replacement: '${2}-${1}'
      - action: replace
        source_labels: [id]
        regex: '^/system\.slice/(.+)\.service$'
        target_label: systemd_service_name
        replacement: '${1}'

